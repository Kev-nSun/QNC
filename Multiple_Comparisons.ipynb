{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh0jzK2J22WO"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PennNGG/Quantitative-Neuroscience/blob/master/Concepts/Python/Multiple%20Comparisons.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKIiY6p3GRFq"
      },
      "source": [
        "# Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7VmLUr5GTNw"
      },
      "source": [
        "The multiple comparisons problem in statistics occurs when multiple statistical inferences are done simultaneously, which greatly increases the probability that any one inference will yield an erroneous result, by chance. A lot has been written about this problem, including:\n",
        "\n",
        "- [Its prevalence in fMRI data analysis](https://www.sciencedirect.com/science/article/pii/S1053811912007057?via%3Dihub) (including a compelling illustration by this [prizewinning study](https://blogs.scientificamerican.com/scicurious-brain/ignobel-prize-in-neuroscience-the-dead-salmon-study/)\\)\n",
        "\n",
        "- [How Baysian methods can avoid the problem](http://www.stat.columbia.edu/~gelman/research/published/multiple2f.pdf).\n",
        "\n",
        "- [General approaches for correcting for multiple comparisons](http://www.biostathandbook.com/multiplecomparisons.html).\n",
        "\n",
        "Here we will provide some intuition for the problem using a simple thought experiment, to sensitize you to how much of a problem it can be. Consider performing the same statistical test on *N* different samples corresponding to, say, different voxels in fMRI data, using a *p*-value of $\\alpha$ (typically 0.05) for each test. \n",
        "\n",
        "Thus, for any one test, the probability of getting a Type I error (rejecting $H_0$ when $H_0$ is true) is $\\alpha$:\n",
        "\n",
        "$p_{error}=\\alpha$\n",
        "\n",
        "For two tests, the probably of getting a Type I error for either test is just one minus the combined probability of not getting a Type I error from either one:\n",
        "\n",
        "$p_{error}=1-(1-\\alpha)(1-\\alpha)$\n",
        "\n",
        "For *N* tests, the probably of getting a Type I error for either test is just one minus the combined probability of not getting a Type I error from any one:\n",
        "\n",
        "$p_{error}=1-(1-\\alpha)^N$\n",
        "\n",
        "Run the cell below to see that the probability of getting a Type I error under these conditions grows rapidly with *N*, implying that it becomes very, very likely that you will get a \"statistically significant result\" just by chance if you do enough tests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "2hOkXUd67ulE",
        "outputId": "23f635b4-bde3-4d29-b916-403fdfb3cce2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "alpha = 0.05\n",
        "N = np.arange(0,100)\n",
        "plt.plot(N, 1-(1-alpha)**N)\n",
        "plt.xlabel('N')\n",
        "plt.ylabel('$P_{error}$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzkMd-fc9cVM"
      },
      "source": [
        "# Correcting for multiple comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncsvEVZB9gNS"
      },
      "source": [
        "\n",
        "\n",
        "There are a number of different methods that can be used to [correct for this problem](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5506159/). Below are two common methods.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7_LgTOk9sPr"
      },
      "source": [
        "## Bonferroni correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N0vdWBq9vNU"
      },
      "source": [
        "The simplest way to correct for Type I errors (false positives) in multiple comparisons is to divide $\\alpha$ by the number of comparisons, known as the [Bonferroni correction](https://mathworld.wolfram.com/BonferroniCorrection.html). This is a very conservative test that is typically used when the number of comparisons is relatively small and you want to avoid Type I errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YxAlI859900"
      },
      "source": [
        "## Benjamini–Hochberg procedure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSE9u8cN-A9U"
      },
      "source": [
        "Another approach is to more carefully control the false-discovery rate using the [Benjamini–Hochberg procedure](https://www.jstor.org/stable/2346101?seq=1#metadata_info_tab_contents):\n",
        "\n",
        "1\\. Rank the individual *p*-values in ascending order, labeled *i*=1...*n*\n",
        "\n",
        "2\\. For each *p*-value, calculate its \"critical value\" as (*i*/*n*)*Q*, where *i* is the rank, *n* is the total number of tests, and *Q* is the false discovery rate (a percentage) that you choose (typically 0.05).\n",
        "\n",
        "3\\. In your rank-ordered, original *p*-values, find the largest value that is smaller than its associated critical value; this *p*-value is the new criterion (i.e., reject $H_0$ for all cases for which *p* ≤ this value)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnW-Xaa422Wh"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZwpKcuE22Wi"
      },
      "source": [
        "In this exercise we will run through an example of correcting for multiple comparisons with both the Benjamini-Hochberg procedure and the more conservative Bonferroni correction. \n",
        "\n",
        "First, simulate multiple (say, 1000) t-tests comparing two samples with equal means and standard deviations, and save the p-values. Obviously, at p<0.05 we expect that ~5% of the simulations to yield a \"statistically significant\" result (of rejecting the NULL hypothesis that the samples come from distributions with equal means).\n",
        "\n",
        "Second, once you have the simulated p-values, apply both methods to address the multiple comparisons problem.\n",
        "\n",
        "Third, set the sample 1 and sample 2 means to be 1 and 2 respectively, and re-run the exercise. What do you notice? What if you make the difference between means even greater?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2XWCZIo7VWJ",
        "outputId": "f925ba54-a771-4dd1-89f4-b506ad05f9e7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "\n",
        "# Define unpaired measurements, same sigma, same mu\n",
        "mu = 1\n",
        "sigma = 1\n",
        "sig_counter = 0\n",
        "\n",
        "# 1000 simulations of t tests\n",
        "for n in range(1000):\n",
        "  # Get random samples, same n\n",
        "  N = 10\n",
        "  X1 = np.random.normal(mu, sigma, N)\n",
        "  X2 = np.random.normal(mu, sigma, N)\n",
        "\n",
        "  # Calculate stats using ttest (use ttest_ind for two independent samples)\n",
        "  tstat, pval = st.ttest_ind(X1, X2)\n",
        "\n",
        "  # Count sig p values\n",
        "  if pval <= 0.05:\n",
        "    sig_counter = sig_counter + 1\n",
        "\n",
        "# Calculate percent of sig results out of 1000\n",
        "percent_sig = (sig_counter/1000)*100\n",
        "print(f'For two normal distributions with the same parameters, the proportion of \"statistically significant\" t-test results out of 1000 is {percent_sig:.2f}%.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbqwpM1G_dsc",
        "outputId": "e985b250-8e70-46e2-d16c-cfef56906198"
      },
      "outputs": [],
      "source": [
        "# Define unpaired measurements, same sigma, same mu, 1000 simulations\n",
        "mu = 1\n",
        "sigma = 1\n",
        "sig_counter = 0\n",
        "sim = 1000\n",
        "\n",
        "# N simulations of t-tests with Bonferroni \n",
        "for n in range(sim):\n",
        "  # Get random samples, same n\n",
        "  N = 10\n",
        "  X1 = np.random.normal(mu, sigma, N)\n",
        "  X2 = np.random.normal(mu, sigma, N)\n",
        "\n",
        "  # Calculate stats using ttest (use ttest_ind for two independent samples)\n",
        "  tstat, pval = st.ttest_ind(X1, X2)\n",
        "\n",
        "  # Count sig p values with Bonferroni\n",
        "  alpha = 0.05/sim\n",
        "  if pval <= alpha:\n",
        "    sig_counter = sig_counter + 1\n",
        "\n",
        "# Calculate percent of sig results out of 1000\n",
        "percent_sig = (sig_counter/sim)*100\n",
        "print(f'For two normal distributions with the same parameters, the proportion of \"statistically significant\" t-test results with Bonferroni correction out of {sim} is {percent_sig:.2f}%.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2dr3WUlAmjv",
        "outputId": "247c7655-cf03-457d-9682-d13a71f85e95"
      },
      "outputs": [],
      "source": [
        "# Define unpaired measurements, same sigma, same mu, 1000 simulations\n",
        "mu = 1\n",
        "sigma = 1\n",
        "p_vals = []\n",
        "sig_counter = 0\n",
        "sim = 1000\n",
        "Q = 0.05\n",
        "\n",
        "# N simulations of t-tests\n",
        "for n in range(sim):\n",
        "  # Get random samples, same n\n",
        "  N = 10\n",
        "  X1 = np.random.normal(mu, sigma, N)\n",
        "  X2 = np.random.normal(mu, sigma, N)\n",
        "\n",
        "  # Calculate stats using ttest, add p val to list\n",
        "  tstat, pval = st.ttest_ind(X1, X2)\n",
        "  p_vals.append(pval)\n",
        "\n",
        "# Sort p values from largest to smallest\n",
        "p_vals.sort(reverse=True)\n",
        "\n",
        "# Calculate critical value for each p value\n",
        "for n in range(sim):\n",
        "  crit_val = ((n+1)/sim)*Q\n",
        "  # Detemrine largest p value that is smaller than associated critical value, set a new p value\n",
        "  if p_vals[n] < crit_val:\n",
        "    new_p = p_vals[n]\n",
        "    break\n",
        "\n",
        "print(f'The new p value determined by Benhamini_Hochberg procedure is {new_p:.4f}.')\n",
        "\n",
        "# Count number of p values <= new p value\n",
        "for n in range(sim):\n",
        "  if p_vals[n] <= new_p:\n",
        "    sig_counter = sig_counter + 1\n",
        "\n",
        "# Calculate percent of sig results out of 1000\n",
        "percent_sig = (sig_counter/sim)*100\n",
        "\n",
        "print(f'For two normal distributions with the same parameters, the proportion of \"statistically significant\" t-test results out of {sim} is {percent_sig:.2f}%.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWUhJpe6KJzX",
        "outputId": "93320283-71eb-48a5-ace8-53779e6aaf8c"
      },
      "outputs": [],
      "source": [
        "# Define unpaired measurements, same sigma, -different mu\n",
        "mu_one = 1\n",
        "mu_two = 2\n",
        "sigma = 1\n",
        "sim = 1000\n",
        "sig_counter = 0\n",
        "\n",
        "# 1000 simulations of t tests, no correction\n",
        "for n in range(sim):\n",
        "  # Get random samples, same n\n",
        "  N = 10\n",
        "  X1 = np.random.normal(mu_one, sigma, N)\n",
        "  X2 = np.random.normal(mu_two, sigma, N)\n",
        "\n",
        "  # Calculate stats using ttest (use ttest_ind for two independent samples)\n",
        "  tstat, pval = st.ttest_ind(X1, X2)\n",
        "\n",
        "  # Count sig p values\n",
        "  if pval <= 0.05:\n",
        "    sig_counter = sig_counter + 1\n",
        "\n",
        "# Calculate percent of sig results out of 1000\n",
        "percent_sig = (sig_counter/sim)*100\n",
        "print(f'For two normal distributions with a mean of 1 and 2 and a shared sigma of 1, the proportion of \"statistically significant\" t-test results out of {sim} is {percent_sig:.2f}%.\\n')\n",
        "\n",
        "\n",
        "\n",
        "# N simulations of t-tests with Bonferroni \n",
        "sig_counter_b = 0\n",
        "\n",
        "for n in range(sim):\n",
        "  # Get random samples, same n\n",
        "  N = 10\n",
        "  X1 = np.random.normal(mu_one, sigma, N)\n",
        "  X2 = np.random.normal(mu_two, sigma, N)\n",
        "\n",
        "  # Calculate stats using ttest (use ttest_ind for two independent samples)\n",
        "  tstat, pval = st.ttest_ind(X1, X2)\n",
        "\n",
        "  # Count sig p values with Bonferroni\n",
        "  alpha = 0.05/sim\n",
        "  if pval <= alpha:\n",
        "    sig_counter_b = sig_counter_b + 1\n",
        "\n",
        "# Calculate percent of sig results out of 1000\n",
        "percent_sig = (sig_counter_b/sim)*100\n",
        "print(f'The proportion of \"statistically significant\" t-test results with Bonferroni correction out of {sim} is {percent_sig:.2f}%.\\n')\n",
        "\n",
        "\n",
        "\n",
        "# Calculate with Benhamini_Hochberg\n",
        "p_vals = []\n",
        "sig_counter_BH = 0\n",
        "Q = 0.05\n",
        "\n",
        "# N simulations of t-tests\n",
        "for n in range(sim):\n",
        "  # Get random samples, same n\n",
        "  N = 10\n",
        "  X1 = np.random.normal(mu_one, sigma, N)\n",
        "  X2 = np.random.normal(mu_two, sigma, N)\n",
        "\n",
        "  # Calculate stats using ttest, add p val to list\n",
        "  tstat, pval = st.ttest_ind(X1, X2)\n",
        "  p_vals.append(pval)\n",
        "\n",
        "# Sort p values from largest to smallest\n",
        "p_vals.sort(reverse=True)\n",
        "\n",
        "# Calculate critical value for each p value\n",
        "for n in range(sim):\n",
        "  crit_val = ((n+1)/sim)*Q\n",
        "  # Detemrine largest p value that is smaller than associated critical value, set a new p value\n",
        "  if p_vals[n] < crit_val:\n",
        "    new_p = p_vals[n]\n",
        "    break\n",
        "\n",
        "print(f'The new p value determined with Benhamini_Hochberg procedure is {new_p:.4f}.')\n",
        "\n",
        "# Count number of p values <= new p value\n",
        "for n in range(sim):\n",
        "  if p_vals[n] <= new_p:\n",
        "    sig_counter_BH = sig_counter_BH + 1\n",
        "\n",
        "# Calculate percent of sig results out of 1000\n",
        "percent_sig = (sig_counter_BH/sim)*100\n",
        "\n",
        "print(f'The proportion of \"statistically significant\" t-test results out of {sim} is {percent_sig:.2f}%.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqoNXyuxP-go"
      },
      "source": [
        "# Additional Resources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clnBO4FU28El"
      },
      "source": [
        "How to correct for multiple comparisons in [Matlab](https://www.mathworks.com/help/stats/multcompare.html), [R](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/p.adjust.html), and [Python](https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tteEm2Qlgbb3"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Copyright 2021 by Joshua I. Gold, University of Pennsylvania"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pKIiY6p3GRFq"
      ],
      "name": "Multiple Comparisons",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
